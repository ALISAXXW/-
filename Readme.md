## Backdoors in Federated Learning

### Attacks
Constrain-and-scale(AISTATS 20)(C 类会议) [Paper](https://proceedings.mlr.press/v108/bagdasaryan20a.html) [Code]https://github.com/ebagdasa/backdoor_federated_learning]
DBA(ICLR 20) [Paper](https://openreview.net/forum?id=rkgyS0VFvr) [Code](https://github.com/AI-secure/DBA)
Edge-Case and PGD(Two attacks)(NIPS 20) [Paper](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html) [Code](https://github.com/ksreenivasan/OOD_Federated_Learning)
Untarget Poisoning(USENIX 20) [Paper](https://www.usenix.org/conference/usenixsecurity20/presentation/fang) [Code](https://people.duke.edu/~zg70/code/fltrust.zip)


### Defenses

FLAME: Taming Backdoors in Federated Learning [Paper](https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen)

Krum(NIPS 17)(Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent.) [Paper](https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html) [Code?](https://github.com/yjlee22/byzantinefl)
AFA(2019 预印)(Auror: defending against poisoning attacks in collaborative deep learning systems)  [Paper](https://dl.acm.org/doi/10.1145/2991079.2991125)
FoolsGold(RAID 20)(The limitations of federated learning in sybil settings) [Paper](https://www.usenix.org/conference/raid2020/presentation/fung) [Code](https://github.com/DistributedML/FoolsGold)
Median(ICML 2018)(Byzantine-robust distributed learning: Towards optimal statistical rates) [Paper](https://proceedings.mlr.press/v80/yin18a.html) [Code?](https://github.com/yjlee22/byzantinefl)
